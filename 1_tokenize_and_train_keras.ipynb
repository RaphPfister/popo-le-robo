{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU, BatchNormalization, Attention\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LambdaCallback, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be 2.1.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda stopped working for some reason, we set number of parallel threads to 2 to avoid CPU over use\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your path to your telegram export csv\n",
    "df = pd.read_csv('telegram_data_20200505.csv')\n",
    "print(len(df))\n",
    "\n",
    "# Removing anything non-text, like stickers\n",
    "df = df.dropna(subset=['text'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Check why this conversion doesn't work\n",
    "# df['text'] = df['text'].str.replace('[0-9]{6}', '######')\n",
    "# df['text'] = df['text'].str.replace('[0-9]{5}', '#####')\n",
    "# df['text'] = df['text'].str.replace('[0-9]{4}', '####')\n",
    "# df['text'] = df['text'].str.replace('[0-9]{3}', '###')\n",
    "\n",
    "# Encoding end of sequence to stop prediction\n",
    "df['text'] = df['text']+ \" <EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting hashtags\n",
    "df['text'] = df['text'].str.replace(\"\\{'type': 'hashtag', 'text': '\", '')\n",
    "df['text'] = df['text'].str.replace(\"'\\}\", '')\n",
    "# Formatting urls\n",
    "reg = re.compile(r\"\\{'type': 'link', 'text': '.*'\\}\")\n",
    "df['text'] = df['text'].str.replace(\"\\{'type': 'link', 'text': '.*\", '', regex=True)\n",
    "# Formatting mentions\n",
    "df['text'] = df['text'].str.replace(\"\\{'type': 'mention_name', 'text': '\", '@')\n",
    "df['text'] = df['text'].str.replace(\", 'user_id': .*\\}\", '')\n",
    "\n",
    "df['text'] = df['text'].str.replace(\"\\{'type': 'mention', 'text': '\", '')\n",
    "df['text'] = df['text'].str.replace(\"'\\}\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text'].str.contains('{')]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting everything that remains stored as list by telegram to plain string\n",
    "df['text'] = df['text'].str.replace('\\[', '')\n",
    "df['text'] = df['text'].str.replace('\\]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : rework this to handle punctuation properly\n",
    "# for punct in \".,!?'\":\n",
    "#     df['text'] = df['text'].str.replace(punct, ' {}'.format(punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].str.split().map(len)>1]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_before = '!\"$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n«»’'+\"'\"\n",
    "# filter belows does not remove punctuation\n",
    "# filter_after = '$%&\"*+-/=?@[\\\\]()^_`{|}~\\t\\n«»’'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a tokenizer on the complete, uncleared vocabulary (because I like a bot that makes typos).\n",
    "End of a sequence are encoded as \\<EOS\\> to help prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None, filters=filter_before, lower=True, split=' ', char_level=False, oov_token='<UNK>', document_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['text'].map(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df['text'].map(str))\n",
    "df['sequences'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : test removing unique word\n",
    "\n",
    "# count_thres = 1\n",
    "# low_count_words = [w for w,c in tokenizer.word_counts.items() if c <= count_thres]\n",
    "# removed = []\n",
    "# for w in low_count_words:\n",
    "#     removed.append(w) \n",
    "#     del tokenizer.word_index[w]\n",
    "#     del tokenizer.word_docs[w]\n",
    "#     del tokenizer.word_counts[w]\n",
    "# print(\"removed : \", len(removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Keras does not handle very well variable length sequences. We need to pad tokenized sequences to 0s\n",
    "Don't forget to add Embedding parameter mask_zero=True to ensure the NN ignores padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "padded_sequences = pad_sequences(sequences, maxlen=20)\n",
    "for seq in padded_sequences:\n",
    "    X.append(np.array([0]+list(seq[:-1])))\n",
    "    y.append(seq)\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 100, input_length=None, mask_zero=True))\n",
    "model.add(LSTM(30, return_sequences=True))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a, temperature=500):\n",
    "    preds = np.asarray(a).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "A = 0\n",
    "\n",
    "def generate(seed, length, n):\n",
    "    print(\"SEED: \",seed)\n",
    "    n = min(n, 150)\n",
    "    seed_tk = tokenizer.texts_to_sequences([seed])\n",
    "    for i in range(length):\n",
    "        y_pred = model.predict(seed_tk)\n",
    "        next_words_proba = y_pred[0][-1]\n",
    "        best_n_next = next_words_proba.argsort()[-n:]\n",
    "        \n",
    "        #print(\"BEST: \", tokenizer.index_word[best_n_next[0]])\n",
    "        #print(\"BEST n: \", [tokenizer.index_word[best_n_next[i]] for i in range(n)])\n",
    "        next_word = np.random.choice(best_n_next)\n",
    "        y_text = tokenizer.index_word[next_word]\n",
    "        if y_text == '<eos>':\n",
    "            return seed\n",
    "        seed = seed + ' ' + y_text\n",
    "        seed_tk = tokenizer.texts_to_sequences([seed])\n",
    "    print(seed)\n",
    "    for punct in '.,!?()\"'+'\"':\n",
    "        seed = seed.replace(' {}'.format(punct), punct)\n",
    "    return seed\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(word_splits) - maxlen - 1)\n",
    "    sentence = ' '.join(word_splits[start_index: start_index + maxlen])\n",
    "\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "    #s = \"Je suis\"\n",
    "    s = sentence\n",
    "    token = tokenizer.texts_to_sequences([s])\n",
    "    print('----- Generated: \"' + generate(s, n=5, length=20) + '\"')\n",
    "    print('----- Generated baseline : \"' + generate(\"Je suis\", n=5, length=20) + '\"')\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.hdf5', monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, mode='min',\n",
    "                              patience=1, min_lr=0.000001 , verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=2, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks = [checkpoint, reduce_lr, early_stopping, print_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : because we test the model on each epoch in an iterative manner, tensorflow retracing function may return a hugh number of warnings\n",
    "# Training goes for approx an hour (on 2 threads)\n",
    "model.fit(X_train,\n",
    "          y_train.reshape(y_train.shape[0], y_train.shape[1], 1),\n",
    "          batch_size=512,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(X_test, y_test.reshape(y_test.shape[0], y_test.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['val_loss'], label='val')\n",
    "plt.plot(model.history.history['loss'], label='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick routine to save, and to make sure everything has been saved properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded = load_model(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
